//! # Language macros
//!
//! Provides the [`languages!`] macro to include deserialized languages and language DFA maps
#![cfg_attr(
    docs,
    feature(doc_auto_cfg),
    deny(rustdoc::broken_intra_doc_links, missing_docs)
)]
#![cfg_attr(not(docs), warn(rustdoc::broken_intra_doc_links, missing_docs))]
#![allow(rustdoc::private_intra_doc_links)]

mod se;

use crate::se::Serialize;
use lang_inner::LanguageRaw;
use lang_inner::{
    compress::{UStream, Compress},
    tables::tabularize,
};
#[cfg(feature = "table-page")]
use table_page::display;
use proc_macro::TokenStream as pmTS;
use proc_macro2::Span;
use quote::{quote, ToTokens};
use std::path::PathBuf;
use syn::{parse::Parser, punctuated::Punctuated, Error, Ident, LitStr, Token, parse_macro_input, ItemEnum, Attribute, AttrStyle, Meta, MetaNameValue, Path, PathSegment, Expr, ExprLit, Lit};

#[proc_macro]
/// # Language inclusion macro
///
/// Provides deserialized languages in their own module.
///
/// ## Usage
///
/// ```ignore
/// languages!(file1, file2, ...)
/// ```
///
/// ## Produces
///
/// ```ignore
/// mod language_code {
/// 	pub const LANG: LanguageRaw = /* .. */;
/// 	pub const MAP: (dyn Table, dyn Table, dyn Table) = /* .. */;
/// }
/// pub fn get(l: &str) -> Option<(&LanguageRaw, &(Box<dyn Table>, Box<dyn Table>, Box<dyn Table>))> {
/// 	/* ... */
/// }
pub fn languages(mods: pmTS) -> pmTS {
    let mut modules = Vec::new();
    let mut arms = Vec::new();
	
	let mut base = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
	base = base.parent().unwrap().to_path_buf();
	base = base.join("languages");
	
	#[cfg(feature = "table-page")]
	let mut languages_table = Vec::new();

    for module in Punctuated::<Ident, Token![,]>::parse_terminated
		.parse(mods)
		.expect("Could not parse input")
    {
		let mod_str = module.to_string();
		let inner = match std::fs::read_to_string(base.join(format!("{}.fckl", mod_str))) {
			Ok(inner) => inner,
			Err(e) => return pmTS::from(Error::new(module.span(), format!("Unable to read language file {}.fckl:\n{:?}", mod_str, e)).to_compile_error())
		};
        let lang =
            LanguageRaw::from_text(&*inner).expect(&*format!("Unable to parse {}", mod_str));
		if lang.is_invalid() {
			return pmTS::from(Error::new(module.span(), format!("Language {} failed verification", mod_str)).to_compile_error())
		}
        let doc1 = format!(
            "Autogenerated language module for {} ({})",
            lang.name.0, lang.name.1
        );
        let doc2 = format!("RawLanguage for {}", lang.name.0);
        let mut doc3 = format!(
            "Lexer map for {}\n\nOriginal densities:",
            lang.name.0
        );
        let (transition, tt, td) = tabularize(&lang);
		#[cfg(feature = "table-page")]
		languages_table.push(((lang.name.0.to_string(), lang.name.1.to_string()), transition.clone(), tt.clone(), td.clone()));
		let transition_density = transition.iter().fold((0u64, 0u64), |acc, v| {
			let row = v.iter().fold((0u64, 0u64), |(total, non_zero), t| if *t == 0 { (total + 1, non_zero) } else { (total + 1, non_zero + 1) });
			(acc.0 + row.0, acc.1 + row.1)
		});
		doc3 += &*format!("\n- Transition: {}/{}={:08.5}% ({})", transition_density.1, transition_density.0, transition_density.1 as f64 / transition_density.0 as f64 * 100., transition.len() * 256 * 4);
		let transition_density = tt.iter().fold((0u64, 0u64), |acc, v| {
			let row = v.iter().fold((0u64, 0u64), |(total, non_zero), t| if *t == 0 { (total + 1, non_zero) } else { (total + 1, non_zero + 1) });
			(acc.0 + row.0, acc.1 + row.1)
		});
		doc3 += &*format!("\n- TT: {}/{}={:08.5}% ({})", transition_density.1, transition_density.0, transition_density.1 as f64 / transition_density.0 as f64 * 100., tt.len() * 256 * 2);
		let transition_density = td.iter().fold((0u64, 0u64), |acc, v| {
			let row = v.iter().fold((0u64, 0u64), |(total, non_zero), t| if *t == 0 { (total + 1, non_zero) } else { (total + 1, non_zero + 1) });
			(acc.0 + row.0, acc.1 + row.1)
		});
		doc3 += &*format!("\n- TD: {}/{}={:08.5}% ({})", transition_density.1, transition_density.0, transition_density.1 as f64 / transition_density.0 as f64 * 100., td.len() * 256 * 2);

        /// table compression based on compressed size
        ///
        /// Call with `compress_tables!((table, inner_type, type_token_stream, value_token_stream),*)`
        macro_rules! compress_tables {
		    ($(($t:ident, $i:ty, $ty:ident, $map:ident)),*$(,)?) => {
				$(compress_tables!(@inner, $t, $i, $ty, $map);)*
			};
			(@inner, $t:ident, $i:ty, $ty:ident, $map:ident) => {
				let $ty;
				let $map;
				let ustream = UStream::compress(&$t);
				let stream_len = ustream.stream.len();
				let offsets_len = ustream.offsets.len();
				$ty = quote!{
					UStream<$i, [$i; #stream_len], [u16; #stream_len], [usize; #offsets_len]>
				};
				$map = ustream.into_token_stream();
				doc3 += &*format!("\n- {} {}u8 {}u16", stringify!($t), stream_len * 3 + offsets_len * 8, stream_len * 4 + offsets_len * 8);
			};
		}

        compress_tables! {
            (transition, u16, ty1, map1),
            (tt, u8, ty2, map2),
            (td, u8, ty3, map3),
        }

        let lr = lang.serialize().to_token_stream();
        modules.push(quote! {
            #[automatically_derived]
            mod #module {
                #![doc= #doc1]
                use lang_inner::{prelude::*, compress::UStream};

                #[doc= #doc2]
                pub const LANG: LanguageRaw = #lr;

                #[doc= #doc3]
                pub const MAP: (#ty1, #ty2, #ty3) = (#map1, #map2, #map3);
            }
        });
        let mod_str = LitStr::new(&*mod_str, Span::mixed_site());
        arms.push(quote! {
            #mod_str => return Some((&#module::LANG, (&#module::MAP.0, &#module::MAP.1, &#module::MAP.2))),
        })
    }
	#[cfg(feature = "table-page")]
	match display(languages_table) {
		Ok(()) => {}
		Err(e) => {
			return pmTS::from(Error::new(Span::mixed_site(), format!("Table HTML page gen failed: {}", e)).to_compile_error())
		}
	}
    pmTS::from(quote! {
        #(#modules)*

        use lang_inner::verify::Verification;
		use std::ops::Index;
		
		/// Language storage tuple
		///
		/// This type is used to make code more readable. It's how custom languages are stored once read
		pub type LanguageTuple<'a> = (
			LanguageRaw<'a>,
			(
				UStream<u16, Vec<u16>, Vec<u16>, Vec<usize>>,
				UStream<u8, Vec<u8>, Vec<u16>, Vec<usize>>,
				UStream<u8, Vec<u8>, Vec<u16>, Vec<usize>>,
			),
		);
		
		trait SizedIndex<T>: Index<usize, Output=T> + Sized {}
		
		/// Language reference tuple
		///
		/// This type is used to make code more readable. To reduce the memory requirements, all languages
		/// and maps are passed as references. this is the type returned when getting a language
		pub type LanguageTupleRef<'a> = (&'a LanguageRaw<'a>, (&'a dyn Table<u16>, &'a dyn Table<u8>, &'a dyn Table<u8>));

        /// Get language and map from a language code
		///
		/// Try to get a built-in language from a language code. This will return a
		/// [`LanguageRaw`](lang_inner::LanguageRaw) and a language transition map to be used with
		/// [`tokenize`](crate::lexer::tokenize).
		///
		/// **Note that the language code is made lowercase so `EN` is equivalent to `en`.**
        pub fn get<'a>(l: &str, buf: &'a Vec<LanguageTuple<'a>>) -> Option<LanguageTupleRef<'a>> {
			let l = &*l.to_lowercase();
            match l {
                #(#arms)*
                _ => {}
            }
			if let Some((v, (m1, m2, m3))) = buf.iter().find(|(t, _)| t.name.0 == l) {
				return Some((v, (m1, m2, m3)))
			}
			None
        }
		
		/// Get language with no language buffer. See [`get`]
		pub fn get_no_buffer<'a>(l: &str) -> Option<LanguageTupleRef<'a>> {
			let l = &*l.to_lowercase();
            match l {
                #(#arms)*
                _ => {}
            }
			None
		}
    })
}

#[proc_macro_attribute]
/// Generate simple doc comments to see an equivalent enum variant for a different enum.
///
/// Intended for use on `PreTokType` to link to `TokType`
pub fn doc_see(attr: pmTS, item: pmTS) -> pmTS {
	let attr = parse_macro_input!(attr as Ident);
	let attr = attr.to_string();
	let mut item = parse_macro_input!(item as ItemEnum);
	let doc_ident = Ident::new("doc", Span::mixed_site());
	for i in item.variants.iter_mut() {
		let prefix = if i.attrs.iter().position(|t| t.meta.path().is_ident(&doc_ident)).is_some() {
			"\n\n"
		} else { "" };
		let doc = format!("{}See [`{}::{}`]", prefix, attr, i.ident.to_string());
		i.attrs.push(Attribute {
			pound_token: Default::default(),
			style: AttrStyle::Outer,
			bracket_token: Default::default(),
			meta: Meta::NameValue(MetaNameValue {
				path: Path { leading_colon: None, segments: Punctuated::from_iter(vec![PathSegment {
					ident: doc_ident.clone(),
					arguments: Default::default(),
				}]) },
				eq_token: Default::default(),
				value: Expr::Lit(ExprLit {
					attrs: vec![],
					lit: Lit::Str(LitStr::new(&*doc, Span::mixed_site())),
				}),
			}),
		})
	}
	pmTS::from(item.to_token_stream())
}
